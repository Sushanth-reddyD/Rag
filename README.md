# Customer Support RAG System

> **Intelligent Customer Service System with Query Routing, Vector Retrieval, and AI Generation**

A production-ready Retrieval-Augmented Generation (RAG) system that intelligently routes customer queries, retrieves relevant information from documents, and generates natural language answers using local or cloud-based AI models.

## ğŸ¯ Overview

This system provides an intelligent customer service solution with:

- **Smart Query Routing** - BERT-based classification routes queries to appropriate handlers
- **Vector Retrieval** - Semantic search across documents using embeddings
- **AI Answer Generation** - Natural language responses using Gemma (local) or Gemini (cloud)
- **Web UI** - Beautiful dark-themed Streamlit interface
- **Production-Ready** - Comprehensive error handling, logging, and performance tracking

## âœ¨ Key Features

### 1. Intelligent Router
- **Fine-tuned BERT classifier** for accurate query categorization
- **Four routing categories**: Retrieval, Complaint, API Call, Conversational
- **85.7% classification accuracy** on test set
- **Confidence scoring** and reasoning for transparency

### 2. Vector Retrieval System
- **Semantic search** using sentence-transformers (MiniLM-L6-v2)
- **Hybrid storage** with ChromaDB vector store + JSON metadata
- **Multi-stage retrieval** with re-ranking and score fusion
- **Citation tracking** with source attribution
- **Supports multiple formats**: PDF, HTML, DOCX, TXT

### 3. Flexible AI Generation
- **Local option**: Gemma 3 270M (free, private, CPU/GPU)
- **Cloud option**: Gemini 2.5 (fast, powerful, API-based)
- **Structured JSON output** for reliable parsing
- **Detailed timing metrics** for performance optimization

### 4. Modern Web Interface
- **Dark theme** optimized for readability
- **Real-time responses** with streaming support
- **Source citations** displayed cleanly
- **Chat history** maintained during session

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT Router                â”‚
â”‚  â€¢ Complaint                â”‚
â”‚  â€¢ API Call                 â”‚
â”‚  â€¢ Retrieval  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”
â”‚  â€¢ Conversational           â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                  â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Retrieval Pipeline  â”‚
â”‚  1. Query Processing        â”‚
â”‚  2. Embedding Search        â”‚
â”‚  3. Re-ranking              â”‚
â”‚  4. Citation Enrichment     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Generation              â”‚
â”‚  â€¢ Gemma 3 270M (local)     â”‚
â”‚  â€¢ Gemini 2.5 (cloud)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Formatted Response         â”‚
â”‚  â€¢ Answer text              â”‚
â”‚  â€¢ Source citations         â”‚
â”‚  â€¢ Confidence scores        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- 4GB+ RAM (8GB+ recommended for Gemma)
- Git

### Quick Install

```bash
# Clone repository
git clone https://github.com/Sushanth-reddyD/Rag.git
cd Rag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# (Optional) Set up Gemini API
export GEMINI_API_KEY='your-api-key-here'
```

### Verify Installation

```bash
# Run tests
pytest tests/

# Start web interface
streamlit run app.py
```

## ğŸš€ Quick Start

### 1. Using the Web Interface

```bash
streamlit run app.py
```

Open your browser to `http://localhost:8501` and start asking questions!

**Example queries:**
- "What is your return policy?"
- "How long do products last?"
- "What is the shipping policy?"

### 2. Using Python API

```python
from src.router.orchestrator import LangGraphOrchestrator

# Initialize system
orchestrator = LangGraphOrchestrator(
    use_real_retrieval=True,
    auto_load_docs=True
)

# Query the system
result = orchestrator.route_query("What is your return policy?")

# Access results
print(result['answer'])          # Generated answer
print(result['routing_decision']) # Which category
print(result['sources'])          # Source documents
```

### 3. Switching Between Gemma and Gemini

**Option A: Environment Variables**
```bash
# Use local Gemma (default)
export MODEL_TYPE='gemma'

# Use cloud Gemini
export MODEL_TYPE='gemini'
export GEMINI_API_KEY='your-key'
```

**Option B: Configuration File**

Edit `src/config/model_config.py`:
```python
MODEL_TYPE = 'gemini'  # or 'gemma'
MODEL_ID = 'gemini-2.5-flash'  # or 'google/gemma-3-270m-it'
GEMINI_API_KEY = 'your-key'  # or read from env
```

## ğŸ“š Configuration

### Model Selection

| Model | Speed | Cost | Privacy | Requirements |
|-------|-------|------|---------|--------------|
| **Gemma 3 270M** | Moderate (10-20 tok/s on CPU) | Free | Local | 4GB RAM |
| **Gemini 2.5 Flash** | Fast (100+ tok/s) | ~$0.001/query | Cloud | API key |
| **Gemini 2.5 Pro** | Fast (80+ tok/s) | ~$0.01/query | Cloud | API key |

### Key Configuration Options

**Router Configuration** (`src/config/model_config.py`):
```python
# Model selection
MODEL_TYPE = 'gemma'  # or 'gemini'
MODEL_ID = 'google/gemma-3-270m-it'

# Generation parameters
GEMMA_CONFIG = {
    'max_context_length': 32000,
    'max_new_tokens': 256,
    'temperature': 0.1,
    'device': 'cpu'  # or 'cuda'
}
```

**Retrieval Configuration** (`src/retrieval/pipeline.py`):
```python
config = RetrievalConfig(
    initial_k=20,        # Initial retrieval size
    final_k=5,           # Final results returned
    score_threshold=0.3, # Minimum relevance score
    use_sparse=False,    # Enable BM25 (optional)
    dense_weight=0.7,    # Dense search weight
    sparse_weight=0.3    # Sparse search weight
)
```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest tests/ -v
```

### Run Specific Test Suites
```bash
# Test router
pytest tests/test_router.py -v

# Test retrieval
pytest tests/test_orchestrator.py -v

# Test phase 2 integration
pytest tests/test_phase2.py -v
```

### Test Generation Speed
```bash
python test_gemma_speed.py
```

### Test Embedding Similarity
```bash
python test_embedding_semantic_match.py
```

## ğŸ“ Project Structure

```
Rag/
â”œâ”€â”€ app.py                      # Streamlit web interface
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pytest.ini                  # Test configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ model_config.py     # Model selection & config
â”‚   â”‚   â””â”€â”€ settings.py         # Application settings
â”‚   â”‚
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Main orchestration logic
â”‚   â”‚   â”œâ”€â”€ router_node.py      # BERT routing
â”‚   â”‚   â””â”€â”€ routing_logic.py    # Conditional routing
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ agent.py            # Retrieval agent
â”‚   â”‚   â””â”€â”€ pipeline.py         # Multi-stage retrieval
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â””â”€â”€ model_factory.py    # Gemma & Gemini generators
â”‚   â”‚
â”‚   â”œâ”€â”€ vectordb/
â”‚   â”‚   â””â”€â”€ store.py            # ChromaDB integration
â”‚   â”‚
â”‚   â””â”€â”€ ingestion/
â”‚       â”œâ”€â”€ pipeline.py         # Document ingestion
â”‚       â”œâ”€â”€ preprocessing.py    # Document parsing
â”‚       â””â”€â”€ chunking.py         # Text chunking strategies
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_router.py          # Router tests
â”‚   â”œâ”€â”€ test_orchestrator.py    # Integration tests
â”‚   â””â”€â”€ test_phase2.py          # Phase 2 tests
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ run_orchestrator.py     # Basic usage example
â”‚   â””â”€â”€ visualize_graph.py      # Visualize workflow
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/              # Source documents
â”‚   â””â”€â”€ metadata/               # Chunk metadata (JSON)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ fine_tuned_router/      # Fine-tuned BERT model
â”‚
â”œâ”€â”€ chroma_db/                  # ChromaDB persistence
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ README.md               # This file
    â”œâ”€â”€ QUICKSTART.md           # Quick start guide
    â””â”€â”€ ARCHITECTURE.md         # Technical architecture
```

## ğŸ“Š Performance

### Router Performance
- **Accuracy**: 85.7% on test set
- **Latency**: <500ms per query
- **Memory**: ~1GB (BERT model)

### Retrieval Performance
- **Retrieval time**: 3-5 seconds (initial query)
- **Retrieval time**: <1 second (cached)
- **Precision@5**: ~80% relevant results

### Generation Performance
- **Gemma 3 270M (CPU)**: 10-20 tokens/sec
- **Gemma 3 270M (GPU)**: 40-60 tokens/sec
- **Gemini 2.5 Flash**: 100+ tokens/sec
- **Gemini 2.5 Pro**: 80+ tokens/sec

## ğŸ”§ Troubleshooting

### Common Issues

**1. Import Error: No module named 'src.generation.gemma_generator'**
```bash
# Update to use model_factory instead
from src.generation.model_factory import ModelFactory
```

**2. ChromaDB Connection Error**
```bash
# Delete and rebuild database
rm -rf chroma_db/
python app.py  # Will rebuild automatically
```

**3. Slow Generation on CPU**
```bash
# Switch to Gemini for faster responses
export MODEL_TYPE='gemini'
export GEMINI_API_KEY='your-key'
```

**4. Out of Memory**
```bash
# Reduce context length
# Edit src/config/model_config.py:
GEMMA_CONFIG = {
    'max_context_length': 16000,  # Reduced from 32000
    'max_new_tokens': 128          # Reduced from 256
}
```

## ğŸš¦ Development Roadmap

### Phase 1: Router âœ… (Complete)
- [x] BERT fine-tuning for classification
- [x] Query routing logic
- [x] Test suite
- [x] Performance optimization

### Phase 2: Retrieval âœ… (Complete)
- [x] Document ingestion pipeline
- [x] Vector database integration
- [x] Multi-stage retrieval
- [x] Citation tracking
- [x] Metadata enrichment

### Phase 3: Generation âœ… (Complete)
- [x] Gemma 3 270M integration
- [x] Gemini API integration
- [x] Model factory pattern
- [x] Structured JSON output
- [x] Detailed timing metrics

### Phase 4: UI âœ… (Complete)
- [x] Streamlit web interface
- [x] Dark theme design
- [x] Source citation display
- [x] Chat history

### Phase 5: Future Enhancements
- [ ] Implement Complaint handler agent
- [ ] Implement API Call agent (order tracking, weather)
- [ ] Add conversation memory
- [ ] Multi-turn dialogue support
- [ ] User feedback collection
- [ ] A/B testing framework
- [ ] REST API endpoint
- [ ] Docker containerization
- [ ] Production deployment guide

## ğŸ“– Documentation

- **[README.md](README.md)** - This file: Project overview and setup
- **[QUICKSTART.md](QUICKSTART.md)** - Quick start guide with examples
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Technical architecture and design

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/Rag.git
cd Rag

# Install dev dependencies
pip install -r requirements.txt
pip install pytest black flake8

# Run tests before committing
pytest tests/
black src/
flake8 src/
```

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¤ Author

**Sushanth Reddy**
- GitHub: [@Sushanth-reddyD](https://github.com/Sushanth-reddyD)

## ğŸ™ Acknowledgments

- **Google** - Gemma and Gemini models
- **HuggingFace** - Transformers and model hosting
- **ChromaDB** - Vector database
- **Streamlit** - Web framework


## ğŸ“§ Support

For questions or issues:
- Open an issue on [GitHub](https://github.com/Sushanth-reddyD/Rag/issues)
- Check [QUICKSTART.md](QUICKSTART.md) for common solutions
- Review [ARCHITECTURE.md](ARCHITECTURE.md) for technical details

---

**Built with â¤ï¸ for better customer service**
